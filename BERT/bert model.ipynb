{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=6\n",
    "max_pred=5\n",
    "max_len=30\n",
    "num_hidden=768\n",
    "d_k=d_v=64\n",
    "num_heads=12\n",
    "d_ff=4*num_hidden\n",
    "num_layers=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess:\n",
    "    def __init__(self,corpus) -> None:\n",
    "        self.corpus=corpus\n",
    "    \n",
    "    def _drop_specialchar(self) -> None:\n",
    "        import re\n",
    "        self.sentences = re.sub(\"[.,!?\\\\-]\", '', self.corpus.lower()).split('\\n')\n",
    "    \n",
    "    def _get_worddict(self):\n",
    "        wordset=list(set(' '.join(self.sentences).split()))\n",
    "        self.worddict={'[PAD]': 0, '[CLS]': 1, '[SEP]': 2, '[MASK]': 3}\n",
    "        for index,word in enumerate(wordset):\n",
    "            self.worddict[word]=index+4\n",
    "        self.vocab_size=len(self.worddict)\n",
    "        del wordset\n",
    "    \n",
    "    def _get_sentencetoken(self):\n",
    "        self.sentencetoken=[]\n",
    "        for sentence in self.sentences:\n",
    "            self.sentencetoken.append([self.worddict[word] for word in sentence.split()])\n",
    "    \n",
    "    def build_dataset(self):\n",
    "        self._drop_specialchar()\n",
    "        self._get_worddict()\n",
    "        self._get_sentencetoken()\n",
    "        self.dataset=[]\n",
    "        positive=negative=0\n",
    "        assert not batch_size%2,'batch_size应该是偶数'\n",
    "        import random\n",
    "        while positive!=negative:\n",
    "            sentence_a,sentence_b=random.randrange(len(self.sentences)),random.randrange(len(self.sentences))\n",
    "            token_a,token_b=self.sentencetoken[sentence_a],self.sentencetoken[sentence_b]\n",
    "            input_ids=[self.worddict['[CLS]']]+token_a+[self.worddict['[SEP]']]+token_b+[self.worddict['SEP']]\n",
    "            segment_ids=[0]*(1+len(token_a)+1)+[1]*(len(token_b)+1)\n",
    "            n_pred=min(max_pred,max(1,len(input_ids)*0.15))\n",
    "            position_canbemasked=[i for i,token in enumerate(input_ids) if token != self.worddict['[CLS'] and token != self.worddict['SEP']]\n",
    "            random.shuffle(position_canbemasked)\n",
    "            target_token,masked_position=[],[]\n",
    "            for pos in position_canbemasked[:n_pred]:\n",
    "                masked_position.append(pos)\n",
    "                target_token.append(input_ids[pos])\n",
    "                chance=random.random()\n",
    "                if chance<0.8:\n",
    "                    input_ids[pos]=self.worddict['[MASK]']\n",
    "                elif chance>0.9:\n",
    "                    index=random.randrange(4,self.vocab_size)\n",
    "                    input_ids[pos]=index\n",
    "            n_pad=max_len-len(input_ids)\n",
    "            input_ids.extend([0]*n_pad)\n",
    "            segment_ids.extend([0]*n_pad)\n",
    "            if max_pred>n_pred:\n",
    "                n_pad=max_pred-n_pred\n",
    "                target_token.extend([0]*n_pad)\n",
    "                masked_position.extend([0]*n_pad)\n",
    "            if sentence_a+1==sentence_b and positive<negative:\n",
    "                self.dataset.append([input_ids,segment_ids,target_token,masked_position,True])\n",
    "                positive+=1\n",
    "            elif sentence_a+1!=sentence_b and positive>negative:\n",
    "                self.dataset.append([input_ids,segment_ids,target_token,masked_position,False])\n",
    "                negative+=1\n",
    "        return self.dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawdata=Preprocess(corpus)\n",
    "dataset=rawdata.build_dataset()\n",
    "vocab_size=rawdata.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(Embedding,self).__init__()\n",
    "        self.tok_embed=nn.Embedding(vocab_size,num_hidden)\n",
    "        self.pos_embed=nn.Embedding(max_len,num_hidden)\n",
    "        self.seg_embed=nn.Embedding(2,num_hidden)\n",
    "        self.norm=nn.LayerNorm(num_hidden)\n",
    "\n",
    "    def forward(self,x,seg):\n",
    "        seq_len=x.size(1)\n",
    "        pos=torch.arange(seq_len,dtype=torch.long)\n",
    "        pos=pos.unsqueeze(0).expand_as(x)\n",
    "        embedding=self.tok_embed(x)+self.pos_embed(pos)+self.seg_embed(seg)\n",
    "        return self.norm(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(ScaledDotProductAttention,self).__init__()\n",
    "\n",
    "    def forward(self,Q,K,V,attn_mask):\n",
    "        scores=torch.matmul(Q,K.transpose(-1,-2))/np.sqrt(d_k)\n",
    "        scores.masked_fill_(attn_mask,-1e9)\n",
    "        attn=nn.Softmax(dim=-1)(scores)\n",
    "        context=torch.matmul(attn,V)\n",
    "        return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.W_Q=nn.Linear(num_hidden,d_k*num_heads)\n",
    "        self.W_K=nn.Linear(num_hidden,d_k*num_heads)\n",
    "        self.W_V=nn.Linear(num_hidden,d_v*num_heads)\n",
    "    \n",
    "    def forward(self,Q,K,V,attn_mask):\n",
    "        residual,batch_size=Q,Q.size(0)\n",
    "        q_s=self.W_Q(Q).view(batch_size,-1,num_heads,d_k).transpose(1,2)\n",
    "        k_s=self.W_K(K).view(batch_size,-1,num_heads,d_k).transpose(1,2)\n",
    "        v_s=self.W_V(V).view(batch_size,-1,num_heads,d_v).transpose(1,2)\n",
    "\n",
    "        attn_mask=attn_mask.unsqueeze(1).repeat(1,num_heads,1,1)\n",
    "        context=ScaledDotProductAttention()(q_s,k_s,v_s,attn_mask)\n",
    "        context.transpose(1,2).contiguous().view(batch_size,-1,num_heads*d_v)\n",
    "        output=nn.Linear(num_heads*d_v,num_hidden)(context)\n",
    "        return nn.LayerNorm(num_hidden)(output+residual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gelu(x):\n",
    "    import math\n",
    "    return x*0.5*(1.0+torch.erf(x/math.sqrt(2.0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(PoswiseFeedForwardNet,self).__init__()\n",
    "        self.fc1=nn.Linear(num_hidden,d_ff)\n",
    "        self.fc2=nn.Linear(d_ff,num_hidden)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.fc2(gelu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(EncoderLayer,self).__init__()\n",
    "        self.enc_self_attn=MultiHeadAttention()\n",
    "        self.pos_ffn=PoswiseFeedForwardNet()\n",
    "    \n",
    "    def forward(self,enc_inputs,enc_self_attn_mask):\n",
    "        enc_outputs=self.enc_self_attn(enc_inputs,enc_inputs,enc_inputs,enc_self_attn_mask)\n",
    "        enc_outputs=self.pos_ffn(enc_outputs)\n",
    "        return enc_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attn_pad_mask(seq_q,seq_k):\n",
    "    batch_size,seq_len=seq_q.size()\n",
    "    pad_attn_mask=seq_len.data.eq(0).unsqueeze(1)\n",
    "    return pad_attn_mask.expand(batch_size,seq_len,seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(BERT,self).__init__()\n",
    "        self.embedding=Embedding()\n",
    "        self.layers=nn.ModuleList([EncoderLayer() for _ in range(num_layers)])\n",
    "        self.fc=nn.Sequential(\n",
    "            nn.Linear(num_hidden,num_hidden),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.classifier=nn.Linear(num_hidden,2)\n",
    "        self.linear=nn.Linear(num_hidden,num_hidden)\n",
    "        self.activ2=gelu\n",
    "        embed_weight=self.embedding.tok_embed.weight\n",
    "        self.fc2=nn.Linear(num_hidden,vocab_size,bias=False)\n",
    "        self.fc2.weight=embed_weight\n",
    "\n",
    "    def forward(self,input_ids,segment_ids,masked_pos):\n",
    "        output=self.embedding(input_ids,segment_ids,masked_pos)\n",
    "        enc_self_attn_mask=get_attn_pad_mask(input_ids,input_ids)\n",
    "        for layer in self.layers:\n",
    "            output=layer(output,enc_self_attn_mask)\n",
    "        h_pooled=self.fc(output[:,0])\n",
    "        logits_clsf=self.classifier(h_pooled)\n",
    "        masked_pos=masked_pos[:,:,None].expand(-1,-1,num_hidden)\n",
    "        h_masked=torch.gather(output,1,masked_pos)\n",
    "        h_masked=self.activ2(self.linear(h_masked))\n",
    "        logits_lm=self.fc2(h_masked)\n",
    "        return logits_lm,logits_clsf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=BERT()\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.Adadelta(model.parameters(),lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    for input_ids,segment_ids,masked_tokens,masked_pos,isNext in dataset:\n",
    "        logits_lm,logits_clsf=model(input_ids,segment_ids,masked_pos)\n",
    "        loss_lm=criterion(logits_lm.view(-1,vocab_size),masked_tokens.view(-1))\n",
    "        loss_lm=(loss_lm.float()).mean()\n",
    "        loss_clsf=criterion(logits_clsf,isNext)\n",
    "        loss=loss_lm+loss_clsf\n",
    "        if (epoch+1)%10==0:\n",
    "            print('Epoch:', '%04d' % (epoch + 1), 'loss =', '{:.6f}'.format(loss))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1 (tags/v3.8.1:1b293b6, Dec 18 2019, 23:11:46) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
