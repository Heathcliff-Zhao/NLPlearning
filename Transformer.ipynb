{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class DotProductAttention(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super(DotProductAttention,self).__init__()\n",
    "\n",
    "    def forward(self,q,k,v):\n",
    "        return torch.bmm(torch.bmm(q,k.transpose(1,2))/math.sqrt(q.shape[-1]),v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeshapefine(X,num_head):\n",
    "    X=X.reshape(X.shape[0], X.shape[1], num_head, -1)\n",
    "    X=X.permute(0, 2, 1, 3)\n",
    "    return X.reshape(-1, X.shape[2], X.shape[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reversefineshape(X,num_head):\n",
    "    X=X.reshape(-1,num_head,X.shape[1],X.shape[2])\n",
    "    X=X.permute(0,2,1,3)\n",
    "    return X.reshape(X.shape[0],X.shape[1],-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self,q_len,k_len,v_len,num_hidden,num_head) -> None:\n",
    "        super(MultiHeadAttention,self).__init__()\n",
    "        self.num_head=num_head\n",
    "        self.q=nn.Linear(q_len,num_hidden)\n",
    "        self.k=nn.Linear(k_len,num_hidden)\n",
    "        self.v=nn.Linear(v_len,num_hidden)\n",
    "        self.attention=DotProductAttention()\n",
    "\n",
    "    def forward(self,q,k,v):\n",
    "        return reversefineshape(self.attention(makeshapefine(self.q(q),self.num_head),makeshapefine(self.k(k),self.num_head),makeshapefine(self.v(v),self.num_head)),self.num_head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AddNorm(nn.Module):\n",
    "    def __init__(self,norm_shape) -> None:\n",
    "        super(AddNorm,self).__init__()\n",
    "        self.norm=nn.LayerNorm(norm_shape)\n",
    "    \n",
    "    def forward(self,x,y):\n",
    "        return self.norm(x+y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFFN(nn.Module):\n",
    "    def __init__(self,num_ffn_input,num_ffn_hidden,num_ffn_output) -> None:\n",
    "        super(PositionWiseFFN,self).__init__()\n",
    "        self.dense1=nn.Linear(num_ffn_input,num_ffn_hidden)\n",
    "        self.relu=nn.ReLU()\n",
    "        self.dense2=nn.Linear(num_ffn_hidden,num_ffn_output)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        return self.dense2(self.relu(self.dense1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncodeBlk(nn.Module):\n",
    "    def __init__(self,q_len,k_len,v_len,num_hidden,num_head,norm_shape,num_ffn_input,num_ffn_hidden) -> None:\n",
    "        super(EncodeBlk,self).__init__()\n",
    "        self.attention=MultiHeadAttention(q_len,k_len,v_len,num_hidden,num_head)\n",
    "        self.addnorm1=AddNorm(norm_shape)\n",
    "        self.PWiseFFn=PositionWiseFFN(num_ffn_input,num_ffn_hidden,num_hidden)\n",
    "        self.addnorm2=AddNorm(norm_shape)\n",
    "\n",
    "    def forward(self,x):\n",
    "        y=self.addnorm1(x,self.attention(x,x,x))\n",
    "        return self.addnorm2(y,self.PWiseFFn(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,num_hidden,max_len=1000) -> None:\n",
    "        super(PositionalEncoding,self).__init__()\n",
    "        self.P=torch.zeros((1,max_len,num_hidden))\n",
    "        X=torch.arange(max_len,dtype=torch.float32).reshape(-1,1)/torch.pow(10000,torch.arange(0,num_hidden,2,dtype=torch.float32)/num_hidden)\n",
    "        self.P[:,:,0::2]=torch.sin(X)\n",
    "        self.P[:,:,1::2]=torch.cos(X)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        return X+self.P[:,:X.shape[1],:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(nn.Module):\n",
    "    def __init__(self,num_layer,vocab_size,q_len,k_len,v_len,num_hidden,num_head,norm_shape,num_ffn_input,num_ffn_hidden) -> None:\n",
    "        super(TransformerEncoder,self).__init__()\n",
    "        self.num_hidden=num_hidden\n",
    "        self.embedding=nn.Embedding(vocab_size,num_hidden)\n",
    "        self.posembedding=PositionalEncoding(num_hidden)\n",
    "        self.blocks=nn.Sequential()\n",
    "        for i in range(num_layer):\n",
    "            self.blocks.add_module(\"block\"+str(i),EncodeBlk(q_len,k_len,v_len,num_hidden,num_head,norm_shape,num_ffn_input,num_ffn_hidden))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x=self.posembedding(self.embedding(x)*math.sqrt(self.num_hidden))\n",
    "        self.attention_weights=[None]*len(self.blocks)\n",
    "        for i, block in enumerate(self.blocks):\n",
    "            x=block(x)\n",
    "            self.attention_weights[i]=block.attention.attention.attention_weights\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecodeBlk(nn.Module):\n",
    "    def __init__(self,i,num_ffn_input,num_ffn_hidden,norm_shape,q_len,k_len,v_len,num_hidden,num_head) -> None:\n",
    "        super(DecodeBlk,self).__init__()\n",
    "        self.i=i\n",
    "        self.attention1=MultiHeadAttention(q_len,k_len,v_len,num_hidden,num_head)\n",
    "        self.addnorm1=AddNorm(norm_shape)\n",
    "        self.attention2=MultiHeadAttention(q_len,k_len,v_len,num_hidden,num_head)\n",
    "        self.addnorm2=AddNorm(norm_shape)\n",
    "        self.ffn=PositionWiseFFN(num_ffn_input,num_ffn_hidden,num_hidden)\n",
    "        self.addnorm3=AddNorm(norm_shape)\n",
    "\n",
    "    def forward(self,x,state):\n",
    "        encode_outputs=state[0]\n",
    "        if state[1][self.i] is None:\n",
    "            key_values=x\n",
    "        else:\n",
    "            key_values=torch.cat((state[1][self.i],x),axis=1)\n",
    "        state[1][self.i]=key_values\n",
    "        x2=self.attention1(x,key_values,key_values)\n",
    "        y=self.addnorm1(x,x2)\n",
    "        y2=self.attention2(y,encode_outputs,encode_outputs)\n",
    "        z=self.addnorm2(y,y2)\n",
    "        return self.addnorm3(z,self.ffn(z)),state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerDecoder(nn.Module):\n",
    "    def __init__(self,num_hidden,num_layer,vocab_size,num_ffn_input,num_ffn_hidden,norm_shape,q_len,k_len,v_len,num_head) -> None:\n",
    "        super(TransformerDecoder,self).__init__()\n",
    "        self.num_hidden=num_hidden\n",
    "        self.num_layer=num_layer\n",
    "        self.embedding=nn.Embedding(vocab_size,num_hidden)\n",
    "        self.posembedding=PositionalEncoding(num_hidden)\n",
    "        self.blocks=nn.Sequential()\n",
    "        for i in range(num_layer):\n",
    "            self.blocks.add_module(\"block\"+str(i),DecodeBlk(i,num_ffn_input,num_ffn_hidden,norm_shape,q_len,k_len,v_len,num_hidden,num_head))\n",
    "        self.dense=nn.Linear(num_hidden,vocab_size)\n",
    "    \n",
    "    def init_state(self,encode_outputs):\n",
    "        return [encode_outputs,[None]*self.num_layer]\n",
    "    \n",
    "    def forward(self,x,state):\n",
    "        x=self.posembedding(self.embedding(x)*math.sqrt(self.num_hidden))\n",
    "        for i,block in enumerate(self.blocks):\n",
    "            x,state=block(x,state)\n",
    "        return self.dense(x),state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,encoder,decoder) -> None:\n",
    "        super(Transformer,self).__init__()\n",
    "        self.encoder=encoder\n",
    "        self.decoder=decoder\n",
    "    \n",
    "    def forward(self,encoder_x,decoder_x):\n",
    "        encoder_output=self.encoder(encoder_x)\n",
    "        state=self.decoder.init_state(encoder_output)\n",
    "        decoder_output=self.decoder(decoder_output,state)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hiddens, num_layers, dropout, batch_size, num_steps = 32, 2, 0.1, 64, 10\n",
    "lr, num_epochs, device = 0.005, 200, torch.device('cuda')\n",
    "num_ffn_input, num_ffn_hiddens, num_heads = 32, 64, 4\n",
    "key_size, query_size, value_size = 32, 32, 32\n",
    "norm_shape = [32]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from d2l import torch as d2l\n",
    "train_iter, src_vocab, tgt_vocab = d2l.load_data_nmt(batch_size, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = TransformerEncoder(num_layers,len(tgt_vocab),query_size,key_size,value_size,num_hiddens,num_heads,norm_shape,num_ffn_input,num_ffn_hiddens)\n",
    "decoder = TransformerDecoder(num_hiddens,num_layers,len(tgt_vocab),num_ffn_input,num_ffn_hiddens,norm_shape,query_size,key_size,value_size,num_heads)\n",
    "net = Transformer(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2469a70536e4d2335a2ea8907942d0699c37342a371ac185bdb5b0aa6f073890"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
